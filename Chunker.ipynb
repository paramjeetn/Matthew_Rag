{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ezt7q8R2pk70",
        "outputId": "7faccc4a-506b-4751-8849-afdd96879f0b"
      },
      "outputs": [],
      "source": [
        "#visualization tool for displaying long load/processing times\n",
        "pip install tqdm \n",
        "#data processing\n",
        "pip install pandas \n",
        "#workhorse for converting text into embeddings/vectors\n",
        "pip install sentence-transformers==2.2.2 \n",
        "#data framework for LLM applications\n",
        "pip install llama-index==0.9.29\n",
        "#logging output\n",
        "pip install loguru==0.7.0 \n",
        "#convenient pretty printing library\n",
        "pip install rich \n",
        "#openai Tokenizer library\n",
        "pip install tiktoken "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWrlkm5-qJ_D"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#standard libraries\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import os\n",
        "import time\n",
        "from typing import List, Tuple\n",
        "from math import ceil\n",
        "\n",
        "#external libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rich import print\n",
        "from rich.pretty import pprint #nifty library for pretty printing\n",
        "from torch import cuda\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWfqHM6EpgJ7"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from llama_index.text_splitter import SentenceSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qEAEjcGqsk8"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "collapsed": true,
        "id": "PiorUrgcpeAM",
        "outputId": "db3f971f-89a5-4c62-d223-dd060b62af2c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Instantiate tokenizer for use with ChatGPT-3.5-Turbo\n",
        "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-0125')\n",
        "\n",
        "# Set chunk size (512 tokens) and instantiate your SentenceSplitter\n",
        "chunk_size = 256\n",
        "gpt35_txt_splitter = SentenceSplitter(chunk_size=chunk_size, tokenizer=encoding.encode, chunk_overlap=0)\n",
        "\n",
        "def merge_transcription_text(transcriptions):\n",
        "    merged_text = ' '.join([t['text'].replace('\\n', ' ').replace('\\r', ' ').strip() for t in transcriptions])\n",
        "    return merged_text\n",
        "\n",
        "def chunk_transcriptions(transcriptions, max_tokens, tokenizer):\n",
        "    current_chunk = []\n",
        "    current_chunk_text = \"\"\n",
        "    current_tokens = 0\n",
        "    chunks = []\n",
        "\n",
        "    for t in transcriptions:\n",
        "        tokens = tokenizer(t['text'])\n",
        "        if current_tokens + len(tokens) > max_tokens:\n",
        "            # finalize current chunk\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "            # start new chunk\n",
        "            current_chunk = [t]\n",
        "            current_chunk_text = t['text']\n",
        "            current_tokens = len(tokens)\n",
        "        else:\n",
        "            current_chunk.append(t)\n",
        "            current_chunk_text += \" \" + t['text']\n",
        "            current_tokens += len(tokens)\n",
        "\n",
        "    # append last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_chunks(chunks):\n",
        "    processed_chunks = []\n",
        "    for chunk in chunks:\n",
        "        start_time = chunk[0]['start']\n",
        "        duration = sum(t['duration'] for t in chunk)\n",
        "        text = ' '.join(t['text'].replace('\\n', ' ').replace('\\r', ' ').strip() for t in chunk)\n",
        "        processed_chunks.append({\n",
        "            \"start\": start_time,\n",
        "            \"duration\": duration,\n",
        "            \"text\": text\n",
        "        })\n",
        "    return processed_chunks\n",
        "\n",
        "# Read JSON data from the file\n",
        "with open('raw_dataset.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Assuming data is a list of dictionaries, where each dictionary has a 'content' key\n",
        "video_data = []\n",
        "for item in data:\n",
        "    video_id = item['video_id']\n",
        "    title = item['title']\n",
        "    guest = item['guest']\n",
        "    likes_count = item['likes_count']\n",
        "    chunks = chunk_transcriptions(item['content'], chunk_size, encoding.encode)\n",
        "    transcription_splits = process_chunks(chunks)\n",
        "    video_data.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"title\": title,\n",
        "        \"guest\": guest,\n",
        "        \"likes_count\": likes_count,\n",
        "        \"content\": transcription_splits\n",
        "    })\n",
        "\n",
        "#making content chunks into singular objects in json\n",
        "final_data = []\n",
        "for item in video_data:\n",
        "    video_id = item['video_id']\n",
        "    title = item['title']\n",
        "    guest = item['guest']\n",
        "    likes_count = item['likes_count']\n",
        "    content = item['content']\n",
        "    index=1\n",
        "    for con in content:\n",
        "        doc_id= f\"{item['video_id']}-{str(index).zfill(4)}\"\n",
        "        start_time = con['start']\n",
        "        duration = con['duration']\n",
        "        text = con['text']\n",
        "        final_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"video_id\": video_id,\n",
        "            \"title\": title,\n",
        "            \"guest\": guest,\n",
        "            \"likes_count\": int(likes_count),\n",
        "            \"start\": start_time,\n",
        "            \"duration\": duration,\n",
        "            \"content\": text\n",
        "        })\n",
        "        index+=1\n",
        "\n",
        "\n",
        "\n",
        "# Save the JSON data to a file\n",
        "output_file_path = 'chunked_256_token.json'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "    json.dump(final_data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Data saved to {output_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
